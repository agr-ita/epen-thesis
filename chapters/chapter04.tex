%%%%%%%%%%
% [TODO] %
%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%
% Chapter mini-intro %
%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%% Short BG


%%% Research question + alternatives


%%% Plan


%%%%%%%%%%%%%%%%%%%%%%%%%%
% Articulatory Inversion % %
%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Articulatory Inversion}
\small{\textit{{\color{red}ADD ACKNOWLEDGEMENTS PAUL + EWAN + EMMANUEL.\\}}}

\subsection{Methods}
\subsubsection{Corpora}
\paragraph{MOCHA-TIMIT}
\begin{itemize}
\item General description (type of speech, number of speakers)
\item Articulators
\item Phone set
\item Train-valid-test division (n utterances, time, distribution of phonemes)
\item ...
\end{itemize}

\paragraph{mngu0}
\begin{itemize}
\item General description (type of speech, number of speakers)
\item Articulators: tongue {tip, back, dorsum}, upper lip, lower lip, jaw
\item Phone set
\item Train-valid-test division (n utterances, time, distribution of phonemes)
\item ...
\end{itemize}

\paragraph{mspka}
\begin{itemize}
\item General description (type of speech, number of speakers)
\item Articulators: tongue {tip, back, dorsum}, upper lip, lower lip, jaw
\item Phone set
\item Train-valid-test division (n utterances, time, distribution of phonemes)
\item ...
\end{itemize}

\subsubsection{Features} 
\paragraph{Acoustic}
13 MFCC (c0, not log-energy) + 3 pitch (?) + delta + delta-delta
\paragraph{Articulatory}
{\color{red}Added velum to mngu0 from MOCHA-TIMIT speaker X; added voicing as binary variable...}

\subsubsection{Model architectures} 
\paragraph{Multilayer Perceptron (MLP)}
\paragraph{Mixture Density Network (MDN)}
\paragraph{Trajectory Mixture Density Network (TMDN)}
\paragraph{Recurrent Neural Network (RNN)}
\paragraph{Adversarial {\color{red}blabla}}

\subsubsection{Evaluation metrics}
\paragraph{RMSE}
{\color{red}The RMSE, traditionally use to evaluate ac-art inversion models was computed as blablabla and it represents the deviation in distance between real and reconstructed x and y for articulators blablabla}
\paragraph{ABX}
{\color{red}RMSE traditionally used but what does it mean/how does it translate to what we actually care about? ABX => check contrasts}


\subsection{Results}
\subsubsection{Selecting model architecture}
{\color{red}Performance (RMSE) is better for RNN blabla when training/testing on mngu0 so we keep it for the next steps blabla... \\ Also, check ABX?}

\subsubsection{Data augmentation for mngu0}
{\color{red}From model trained on MOCHA-TIMIT (msak; say why) we reconstruct the velum ([nasal] feature) for mngu0. Blablabla. We tried to reconstruct [voice] from laryngograph but it didn't work because blabla so we train model to predict binary voicing value blabla.}

\subsubsection{Acoustic-articulatory inverter}
{\color{red}Training from MOCHA-TIMIT + mngu0 blabla}


\subsubsection{Robustness across languages}
{\color{red}Test mspka?}


\subsection{Discussion}

BLABLA

%%%%%%%%%%%%%%%%%%%%%%%%% 
% Predicting confusions % %
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Predicting confusion patterns}
\small{\textit{{\color{red}ADD ACKNOWLEDGEMENTS PAUL + EWAN + EMMANUEL + ACKN MILICA.\\}}}

\subsection{Methods}
\subsubsection{Corpora}
\paragraph{Wall Street Journal - Read (WSJ)}
The WSJ \cite{} is a corpus of both read and spontaneous American English. For our work, we only kept to read subset of the corpus.  
\begin{itemize}
\item General description (type of speech, number of speakers)
\item Alignment?
\item Train-test division (n utterances, time)
\item ...
\end{itemize}

\paragraph{Breakfast corpus (MOCHA + mngu0)}
\subsubsection{Stimuli}
%We used the same stimuli as in sections \ref{2-parlato} and \ref{2-parlato-dur}. As a reminder, a native French speaker recorded 54 items with the structure $V_{1}C_{1}C_{2}V_{2}$, with $V_{1}$ and $V_{2}$ vowels from the set \{/a/, /i/, /u/\}, and $C_{1}C_{2}$ a cluster from the set \{/bg/, /bn/, /db/, /dg/, /gb/, /gn/\} (e.g. /abgi/).

\subsubsection{Language models}
Item-specific language models were constructed, as shown in Figure \ref{fig:blabla}. Thus, when decoding a $C_{1}C_{2}V_{1}C_{3}V_{2}$ stimulus, the perception model was only given the possibility to transcribe it as $C_{1}(V_{ep})C_{2}V_{1}C_{3}V_{2}$, where phones between parentheses are optional and $V_{ep} = $ \textipa{[@]}. 

We test {\color{red}X} types of language models (LM) when decoding our items; these differ only in the weights given to edges between nodes {\color{red}X and X} in the graph shown in Figure \ref{fig:blabla}: 
\begin{enumerate}
    \item A null language model (\textsc{0P-LM}), which implies that listeners base their decoding of consonant clusters on phonetics alone, without using information on phonotactics.
\end{enumerate}
{\color{red}The above are copied from another chapter; not all relevant. Maybe. Blabla}

%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.8\linewidth]{chapter04/blabla.pdf}
%\caption{Constrained language model used to test the models (here: LM for \textipa{/znapa/} trials). Nodes in the graph represent states, weighted edges represent transitions between states (here: phonemes). When relevant, eighted edges are labeled with the probability to choose that edge when decoding, which affects the final language model score of each possible path. These scores are then combined with acoustic scores when decoding experimental items. An optional silence can be inserted by the model between states 3 and 4.}
%\label{fig:blabla}
%\end{figure}

\subsubsection{Identification task simulation}
After decoding the stimuli, we obtained for each possible transcription of each item the corresponding acoustic and language model scores. From these we derived the item posteriorgrams, which indicate how probable a given transcription was given the audio input. We used these probabilities as proxies of the probability that a listener might exploit when performing reverse inference during speech perception, and therefore, the probabilities used when responding in an identification task. 

As such, for each item, we obtained a six-dimensional vector $ident_{model} = [p_{none}, p_{a}, p_{e}, p_{i}, p_{o}, p_{u}]$, containing a discrete probability distribution, with a probability mass function linking the identification task options 'none', 'a', 'e', 'i', 'o', 'u', to their respective probabilities (i.e., posteriorgrams).
\subsection{Results}
\subsection{Discussion}

BLABLA

%%%%%%%%%%%%%%%%%%%%%%%%% 
% Perceptual experiment % %
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Perceptual experiment}
\small{\textit{{\color{red}ADD ACKNOWLEDGEMENTS PAUL + EWAN + EMMANUEL.\\}}}
\subsection{Methods}
\subsubsection{Stimuli}
\subsubsection{Participants}
\subsubsection{Procedure}
\subsubsection{Data analysis}
\subsection{Results}
\subsection{Discussion}

BLABLA
%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Chapter mini-discussion %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
%%% Summary

%%% Short discussion

%%% Limitations

%%% Conclusions