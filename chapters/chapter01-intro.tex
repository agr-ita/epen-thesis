
\setlength{\epigraphwidth}{0.7\textwidth}
\epigraph{Luke: Every time you say `Luke', I think you're saying `look'. \\ Gloria: I don't hear the difference. \\ Luke: It's not that hard; one is my name. \\ Gloria: Juan is not your name!}{\textsc{Modern Family}}

Even after spending years learning a foreign language and becoming proficient at speaking it, very often, a dead giveaway that we are indeed not communicating in our own native language is the presence of a foreign accent (e.g., \cite{flege1995factors, munro1996}). For instance, Japanese speakers have difficulties producing the American English consonants \textipa{/\*r/} and \textipa{/l/}, since their native language does not have such a contras of liquid consonant \cite{flege1995rl}. This results in their productions of the words ``right'' and ``light'' being phonetically similar. This merger can also be attested in Japanese loanwords of English origin: the source word ``lion'' has been borrowed as \textipa{/raion/}, while ``sale'' became \textipa{/se:ru/}. In this second loanword there is also evidence of an extra \textipa{/u/} vowel that was not present in the source word. This phenomenon of inserting a vowel to a borrowed loanword is known as vowel epenthesis. It often occurs when the borrowed word does not respect the phonotactics (i.e., legal sound combinations) of the borrowing language (e.g., illegal consonant clusters, illegal syllabification), as in the following examples:
\begin{itemize}
  \item English ``strike'' \textipa{/st\*raIk/} $\rightarrow$ Japanese \textipa{/s\textbf{u}t\textbf{o}raik\textbf{u}/}
  \item French ``baguette'' \textipa{/bagEt/} $\rightarrow$ Japanese \textipa{/baget:\textbf{o}/} %\begin{CJK}{UTF8}{goth}バゲット\end{CJK} \textipa{//} 
  \item English ``snob'' \textipa{/sn6b/} $\rightarrow$ Spanish \textipa{/\textbf{e}snob/}
  \end{itemize}

Nonnative word are imported to the borrowing language in a diachronic process involving word transmission amongst multiple individuals and possibly using various methods of transmission (e.g., through written materials, orally, etc). Additionally, adaptations can be influenced by orthography, when this is available \cite{daland2015,vendelin2006}. However, it is hypothesized that the modifications that we observe in loanwords are not exclusively due to incorrect productions of the nonnative words; both the modifications observed in loanwords and those observed in production are hypothesized to be at least in part due to incorrect \textit{perception} of the nonnative speech \cite{peperkamp2003, peperkamp2005, peperkamp2008, wilson2013,wilson2014}.  How could individuals with different language experiences interpret the same acoustic signal differently?

\section{Nonnative speech misperceptions}

We can study nonnative misperceptions by using several types of experimental paradigms: transcription tasks, classification tasks, discrimination tasks.

In transcription tasks, participants are presented with an auditory stimulus and they are asked to provide a written transcription of what they perceived, either using their native language orthography, or by using a task-specific learned phonetic alphabet (e.g., transcription of items with \textipa{/\*r/} and \textipa{/l/} by Japanese participants in \cite{best1992}). The output of these tasks is difficult to analyse due to the high variability in the responses.

A way to limit these is by turning to classification tasks, where participants are presented with an auditory stimulus and they are asked to assign it to one of multiple given category, therefore limiting the number of possible transcriptions.
A particularly popular variation of this subset of paradigms is the identification task, where participants are asked to identify a specific segment within the stimulus. This task is also known as the \textit{n}-forced choice task (where \textit{n} is the number of possible choices available to the participant. An example of use is \cite{dupoux1999}, where Japanese listeners were asked to say whether they heard a \textipa{/u/} vowel between two consonants in items such as \textipa{/ebzo/}.

Finally, in discrimination tasks participants hear two or more stimuli. They are asked to judge which items belong to the same category. An example is the ABX discrimination task, where two items of different categories (A and B) are presented, followed by a third item that belongs to either A or B. Contrasts that are difficult to perceive result in high error rates. An example is how \cite{durvasula2015} tested epenthesis by Korean listeners in an ABX task where A was an item with no medial vowel (e.g., \textipa{/et\super hma/}, B an item with a medial vowel \textipa{/et\super h1ma/}, and X an item of either category.     \\  

Nonnative misperceptions can be classified in three main categories:

\begin{itemize}
\item Segmental misperceptions: Difficulties perceiving the difference between two nonnative contrasting phonemes. E.g., Japanese listeners' difficulty perceiving the difference between American English \textipa{/\*r/} and \textipa{/l/} \cite{goto1991, miyawaki1981}.
\item Suprasegmental misperceptions: Difficulties perceiving the difference between two nonnative contrasting suprasegments such as lexical stress, lexical tone, pitch accent. E.g., French listeners' difficulty perceiving the difference between Spanish words such as ``líquido'' \textipa{/'likido/} (\textit{liquid}), ``liquido'' \textipa{/li'kido/} (\textit{I liquidate}), ``liquidó'' \textipa{/liki'do/} (\textit{s/he liquidated}) \cite{dupoux1997, dupoux2008}.
\item Phonotactic repairs: Addition (i.e., epenthesis), modification (i.e., adaptation), or deletion (i.e., elipsis) of segments as a strategy to ``fix'' nonnative input which does not conform to native phonotactics. E.g., Word-initial \textipa{/e/}-epenthesis by Spanish speakers in words beginning with an \textipa{/s/} as part of a complex cluster as in ``special'' \cite{halle2014}. Also: Perception of illegal \textipa{/tl/} clusters as \textipa{/kl/} by French listeners \cite{halle2007}.  
\end{itemize}

Interestingly, while they can be minimised with training [CITE lively93 etc], the effects of native phonology on nonnative speech perception are long-lasting [CITE takagi95] and may even be apparent in highly proficient bilinguals [CITE FrEsp].


\section{Perceptual vowel epenthesis}
In this thesis we will focus on a subset of misperceptions resulting from phonotactic repair: perceptual vowel epenthesis. As previously mentioned, we say that listeners experience this phenomenon when they report hearing vowels that are not initially present in the nonnative speech. In cases that will be studied in this work, this seemingly happens as a way to break phonotactically illegal clusters. For instance, in Japanese most consonant clusters%\footnote{Consonant clusters can only be geminates or a nasal followed by another consonant.}
, such as \textipa{/bz/}, are phonotactically illegal. When hearing nonwords containing these clusters, such as \textipa{/ebzo/}, Japanese listeners may reporting hearing an epenthetic \textipa{/u/}\footnote{A more accurate phonetic transcription of the unrounded high back vowel used in Japanese is \textipa{[W]} but, following previous work, the phonological notation \textipa{/u/} will be used in the remainder of the thesis.} within the cluster, yielding \textipa{/ebuzo/} as the percept \cite{dupoux1999}. Epenthesis of \textipa{/u/} by Japanese listeners is not only evident in their behaviour but also in their brain responses; they have difficulties differentiating the clusters produced by a French speaker from their epenthesized counterparts (e.g., \textipa{/ebzo/} \textit{vs.} \textipa{/ebuzo/}) while also showing different event-related potentials compared to native French speakers. The fact that Japanese listeners fail to show sign of MMN (mismatch negativity) in the EEG signal attests that the process of epenthesis occurs early in the process of perception \cite{dehaene2000}. Importantly, experimental data also suggests that epenthesis is a pre-lexical process happening early in speech perception \cite{dupoux2001}.

Perceptual vowel epenthesis has also been attested in languages other than Japanese \cite{dupoux1999, dehaene2000, dupoux2001, monahan2009, dupoux2011, mattingley2015}; indeed, it has also been studied in Korean \cite{kabak2007, shin2011, dejong2012, durvasula2015, durvasula2016}, Brazilian Portuguese \cite{dupoux2011}, Spanish \cite{halle2014}, English \cite{berent2007, zhao2018}, and Mandarin Chinese \cite{durvasula2018}. The quality of the epenthetic vowel depends not only in the language (e.g., \textipa{[W]} in Japanese, \textipa{[i]} in Brazilian Portuguese \cite{dupoux2011}), but also in the phonemic environment (e.g., \textipa{/i/} may be more readily epenthesized in clusters with palatalised consonants). 

In order to estimate phonemic environments in which a listener might experience epenthesis, as well as eventual variations of epenthetic vowel quality, we may turn to loanwords. Since patterns of epenthesis observed in loanword adaptations are, at least in part, due to how the native perceptual system processes the nonnative source word, loanwords can be thought of as fossils from which we can extract hypotheses about online misperceptions. Of course, as we mentioned above loanwords are processed through more than one individual perceptual filter, and can be influenced by orthography, for instance. So, continuing with the fossil metaphore, the online percept is akin to the mosquito trapped in amber after accidentally landing on tree sap; preserved in time but possibly not in mint condition. Large corpora of loanwords exist, which allows us to examine epenthetic vowel patterns: Which phonemic combination trigger vowel epenthesis? Is there a vowel that is predominantly inserted? If so, are other vowels epenthesized in other phonemic environments? Similar to how the paleontologist posits theories about the fauna in ancient times by analysing fossils, the psycholinguist is able to posit theories about perceptual vowel epenthesis from epenthesis in loanwords, and test them empirically.   
   
\section{Processing steps in perceptual vowel epenthesis}

Concerning the process of vowel epenthesis in perception, we can identify two types of proposed pipelines that differ in the amount of processing steps that the nonnative input is subjected to during perception: these are two-step and one-step theories of nonnative speech perception, illustrated in Figure \ref{fig:intro_12step}. While their names are somewhat transparent, we will now explain in more detail the differences between the two types of proposals.   

Two-step theories of nonnative speech perception divide the perception process in two stages. According to these proposals, the quality of the epenthetic vowel is determined by a language-specific grammar after an initial parsing of the nonnative input.
For \cite{berent2007}, the identity of the segments present in the nonnative input is retrieved in an initial step, yielding a \textit{phonetic form}. The native grammar then assesses the phonotactic legality of this phonetic form in a second step. If a phonotactic violation is found, the grammar, which combines both language-specific and universal components, repairs the phonetic form by inserting a vowel. The output of this final step is the \textit{phonological representation}.
Another proposal, that of \cite{monahan2009}, also consists in two steps, but with some differences. During the first step the identity of the segments in the input is retrieved and segments are grouped into syllables, following native phonotactics. Some syllables will contain indeterminate segments (e.g., \textipa{/ebzo/} will have been parsed as \textipa{/e.b}V\textipa{.zo/}). In a second step, the quality of the indeterminate segments, in this case the epenthetic vowel, is chosen amongst vowels that are of low sonority\footnote{Phonemes that are lower in the sonority scale are less audible than higher ranked phonemes. Due to how the tongue is positioned close to the mouth roof during their articulation, high vowels (and glides) are the least sonorous vowels in a vowel inventory.} and can undergo devoicing\footnote{In Japanese, high vowels \textipa{/i/} and \textipa{/u/} can be devoiced in certain contexts, such as between two voiceless segments \cite{han1962,vance1987, tsuchida2001}.}. 
The quality of the vowel might not be determined if an optimal match is not found.
The two proposals that we have summarised share the fact that the categorisation of the segments that are not the epenthetic vowel occurs in a first step and it is not modified during the second step, where the identity of the epenthetic vowel is determined.

\begin{figure}[htb!]
  \centering
  \begin{overpic}[page=1, width=0.9\linewidth]{chapter01-intro/12step}\end{overpic}
  \caption{\textit{Processing of the nonnative stimulus \textipa{/ebzo/} by Japanese listeners, according to two-step and one-step proposals for perceptual vowel epenthesis. From left to right: two-step proposal from \cite{berent2007}, two-step proposal from \cite{monahan2009}, one-step proposal from \cite{dupoux2011,dejong2012,wilson2013}}}
  \label{fig:intro_12step}
\end{figure}

In contrast, by advancing one-step proposals, authors such as \cite{dupoux2011, dejong2012} and \cite{wilson2013} argue that the identity of the epenthetic vowel is determined in the process of parsing the input, simultaneously to the categorisation of all other segments. The phonotactic legality of the input is therefore assessed at the same time as the categorisation happens. Notably, the input is not processed as a linear sequence of sounds; syllabic structure is taken into account during the parsing process \cite{kabak2007}.

\cite{wilson2013} qualify the process as a process of ``reverse inference'' within a Bayesian framework, where the perceptual system computes $P(w | X)$ the posterior probability of candidate percepts $w$ given the auditory input $X$. These are estimated, for each candidate percept, from the product of $P(X|w)$ the likelihood of the acoustics given the percept and $P(w)$ the prior probability of the percept, defined as its phonotactic acceptability. Mathematically, this can be formulated as in equation \ref{eq_onestep1}. Then, in a maximum \textit{a posteriori} (MAP) estimation scenario, the final percept $\widehat{w}$ corresponds to the percept with the highest posterior probability, as shown in equation \ref{eq_onestep2}. Alternatively, the final percept may be estimated by weighted sampling, where weights are defined by the posterior probabilities.     

\begin{equation}
  P(w | X) \propto P(X | w) \cdot P(w)
  \label{eq_onestep1}
\end{equation}

\begin{equation}
  \widehat{w} = \underset{w}{arg\,max} \left \{ P(X|w) \cdot P(w) \right \}
  \label{eq_onestep2}
\end{equation}

In other words, for one-step models, parsing becomes an optimisation problem where the optimal output is the one maximising the acoustic match to the input and the likelihood of the phonemic sequence in the native language. \cite{durvasula2015} add to the aforementioned proposals by suggesting that listeners are decoding nonnative speech through a process of reverse inference that not only optimises the output according to phonetic representations and surface phonotactics, but also according to native phonological alternations (i.e., mappings between underlying and surface representations). What this means is that listeners will also try to infer an underlying phoneme based on the possible surface realisations attached to this phoneme in their native language. For instance, in Korean, \textipa{/s/} surfaces as \textipa{[S]} when in front of the vowel \textipa{/i/}. When hearing a cluster such as \textipa{[Sm]}, listeners may epenthesize \textipa{[i]} after interpreting \textipa{[S]} as the allophone of \textipa{/s/} in front of that vowel. Thus, the extended proposal by \cite{durvasula2015} integrates the involvement of deeper phonological rules/constraints during the perception process.

\section{Modelling approach: An example}
\setlength{\epigraphwidth}{0.4\textwidth}
\epigraph{... But why make models?}{\textit{Derek Zoolander, probably.}}

In this thesis we will evaluate computational implementations of one-step theories of nonnative speech perception. Notably, we will investigate models from the field of automatic speech recognition (ASR) which are a direct implementation of the Bayesian model shown in equation \ref{eq_onestep1}\footnote{The equivalent of a weighted sampling procedure was preferred over MAP estimation for percept selection, since participant responses in previous experimental work on epenthesis tended to show variation and were not deterministic.}. 
While only the one-step family of theories will be evaluated in this work, we encourage further research to be done with similar methodologies in order to investigate all of the various co-existing proposals.

Indeed, using computational models in order to investigate competing theories is beneficial in several ways. Firstly, the need to translate the theories into model implementations forces model ideators to provide a mathematically and/or algorithmically well-defined model. This is in contrast with more vague and ambiguous verbally defined theories that leave more space to reader interpretations. Having more rigorous model definitions also allows us to better understand competing theories (what is the exact nature of the input? which grammar constraints are applied and how? ...), meaning that it is easier to compare proposals and see where they differ significantly or not. 

Secondly, obtaining a computational implementation of a theory means that it is then possible to derive predictions from the models in question. It is then possible to qualitatively and quantitatively examine these predictions, and compare them to what is observed in behavioural data.

For the skeptical reader, possibly left frowning after reading the above statements, we will briefly develop an example of how modelling can allow us to test theories in ways that may be unfeasible otherwise. The specific example, the details of which can be found in Appendix \ref{A-appendix}, is from the literature of developmental psycholinguistics, concerning how acoustic differences in Infant-Directed Speech might or might not promote language learning for infants, compared to Adult-Directed Speech (ADS). Indeed, IDS presents very salient prosodic, lexical, syntactic, and temporal properties (see [CITE Soderstrom2007, Golinkoff2015] for a review).

The hypothesis (which we refer to as the Hyper Learnability Hypothesis; HLH) was advanced by \cite{kuhl1997}. The authors in this study analysed the acoustics of the vowels located at the extremities of the vowel triangle (i.e., \textipa{/i, a, u/}). Analyses showed an increase of the vowel triangle area (in formant space) for IDS compared to ADS. The authors interpreted this as an enhancement of phonemic contrasts, which might help infants identify and acquire phonemic categories more easily. The expansion of the vowel triangle in IDS was also attested in other studies \cite{andruski1999, bernstein1984, burnham2002, cristia2014, liu2003, mcmurray2013, uther2007}, but not systematically across the vowel inventory \cite{cristia2014} and, importantly, IDS presented increased within-category acoustic variability \cite{mcmurray2013, cristia2014, kirchoff2005}. With increased vowel separation and increased within-category variability being opposite effects, we wondered whether the discriminability of IDS phonemes was higher than for ADS vowels or not.

Using a computational model of the ABX discrimination task\footnote{In this task, the discriminability of two categories \textsc{A} and \textsc{B} is assessed by setting triplets of tokens $a$, $b$, and $x$. The first two belong to categories \textsc{A}, \textsc{B}, respectively; the third token belongs to one of the two categories. The algorithm computes the acoustic distance between $a$ and $x$, and $b$ and $x$, and classifies $x$ to the category of the closest token. The more discriminable the two categories in question, the higher the classification accuracy of the algorithm.}, \cite{martin2015} assessed the discriminability of Japanese phonemes in a large corpus of Japanese IDS and ADS. Against expectations, phonemes in IDS were on average less discriminable that in ADS. In the Appendix \ref{A-appendix}, we investigated whether the acoustic and phonological advantage of IDS may surface at level of words. However, we found that words in IDS were also less discriminable than in ADS on average. 

In this set of studies, the modelling approach allowed us to quantitatively and qualitatively test the HLH, in a large scale (several millions of ABX experimental trials simulated), with systematic comparisons of all phonemes/words in the language, without assuming a specific learning algorithm, and using a richer representation of the acoustics based on a model of how speech is processed by the auditory system (as opposed to formant values in other studies). Importantly, modelling allowed us to evaluate the interaction between effects advancing opposing hypotheses, and showed us the resulting predictions in a computationally understandable format. A study of this magnitude would have not been possible with traditional experimental techniques, which makes modelling a welcomed addition to the experimental evidence gathered for and against the HLH.

In an analogous manner, we will be using computational models of nonnative speech perception in this thesis, in order to investigate the underlying mecanisms of perceptual vowel epenthesis in ways that may not be possible when only using behavioural experiments.  

\section{Outline}
Why do people of different linguistic background sometimes perceive the same acoustic signal differently? In particular, how is this nonnative acoustic signal processed to become what the listener ends up perceiving?. How much of this process is guided by the information directly accessible in the acoustic signal? What is the contribution of the native phonology? How are these two elements combined when computing the native percept?

In order to answer these questions, various mechanisms underlying nonnative speech perception have been put forward; however, many lack formal definition that allows them to be tested empirically.  
In this dissertation, we select one of the proposals advanced by the psycholinguistics literature. Namely, we investigate one-step models of nonnative speech perception \cite{dupoux2011, dejong2012, wilson2013, durvasula2015}, which postulate that acoustic match and sequence match between the nonnative input and the native percept are optimised, simultaneously. To do so, we test a proof-of-concept computational implementation of the model as defined by \cite{wilson2013}. 

We present various methodologies for qualitatively and quantitatively evaluating the reverse inference proposal. We do this by focusing on the phenomenon of perceptual vowel epenthesis, namely the phenomenon by which listeners may hallucinate vowels when hearing nonnative speech that does not conform to the structural constraints of their native language. Of interest are both the rates of vowel epenthesis (i.e., how often do participants experience this?) and variations of epenthetic vowel quality (i.e., which vowel is epenthesized?).

Following on the experimental approach recommended by \cite{vendelin2006}, the data arising from the computational models is compared to data from psycholinguistics experiments. In these, nonnative speech perception is evaluated using psycholinguistics paradigms which tap onto online (i.e., real-time, individual) perception of nonwords, in order to reduce the influence of confounds such as orthography and semantics.
In other words, we subject the proposed computational models to tasks analogous to those completed by human participants and analyse their behaviour both quantitatively and qualitatively. Do we find acoustics-based mechanisms to be necessary to predict perceptual vowel epenthesis in human listeners? If so, do they suffice?

This dissertation is divided in two main sections. First, in Chapter 2, we use an identification paradigm to investigate the influence of acoustic details on modulations of epenthetic vowel quality. We discuss the implications of our results in the context of the opposition between the two-step and one-step theories of nonnative speech perception. We find that acoustic details modulate epenthetic vowel quality, results that are in agreement with one-step theories. Building on these results, we present a basic model of speech perception exclusively reliant on acoustic matching betwen minimal pairs of nonnative and native speech exemplars. Namely, we build non-parametric exemplar-based models of perception. Relative to human results, we find the models to be able to reproduce some qualitative effects linked to the role of coarticulation on epenthetic vowel quality; however, the models are limited by their inability to output responses other than those derived from their specific inventory of exemplars.

In Chapter 3 we turn to a parametric implementation of a one-step proposal, using tools from the field of automatic speech recognition (ASR). We present an HMM-GMM speech recognizer composed of independent acoustic and language (i.e., phonotactic) models. These can be tweaked as necessary to test hypotheses about the underlying mechanisms of nonnative speech perception. We propose a novel methodology to test ASR systems which use language models represented by Weighted Finite State Transducers (W-FST) in identification tasks analogous to those used to test human participants. Using this method, we test the predictive power of the acoustic model on patterns of vowel epenthesis. We find that the acoustic model alone better predicts human results than when accompanied by language models, at least when the latter are \textit{n}-gram based phonotactic models with phones as the unit \textit{n}. We further test whether some effects traditionally attributed to phonology may actually be predicted from acoustics alone. Following promising but not perfect results, we propose future research paths for enhancing the methodology and to further investigate the mechanisms underlying nonnative speech perception.  