%%% TO-DO %%%
%
% - [ ] Section about nonnative misperceptions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\epigraphwidth}{0.7\textwidth}
\epigraph{``The Babel fish is small, yellow, leech-like, and probably the oddest thing in the Universe. It feeds on brainwave energy received not from its own carrier, but from those around it. It absorbs all unconscious mental frequencies from this brainwave energy to nourish itself with. It then excretes into the mind of its carrier a telepathic matrix formed by combining the conscious thought frequencies with nerve signals picked up from the speech centres of the brain which has supplied them. The practical upshot of all this is that if you stick a Babel fish in your ear you can instantly understand anything said to you in any form of language.''}{Hitchhiker's Guide to the Galaxy \\ \textsc{Douglas Adams}}

If the reader has had the opportunity to endeavour to speak a foreign language, they may agree that Douglas Adams's Babel fish would be a terrific tool. Setting aside the difficulties posed by lexical, syntactic, and morphological differences between the native and nonnative languages, and zooming in something as seemingly low level as the speech sounds encoded in the acoustics, the task of decoding nonnative speech is not the easiest to overcome. Why is it that, from an identical acoustic signal, two people of different linguistic backgrounds will perceive two different things?  

In particular, there is the question of how this nonnative acoustic signal is processed to arrive to the native percept. How much of this process is guided by the information directly accessible in the acoustic signal? What is the contribution of the native phonology? How are these two elements combined when computing the native percept?

Various mechanisms underlying nonnative speech perception have been put forward; however, many lack formal definition that allow them to be tested empirically.  
In this dissertation, we select one of the proposals advanced by the psycholinguistics literature. Namely, we investigate one-step models of nonnative speech perception \cite{dupoux2011, dejong2012, wilson2013, durvasula2015}, which postulate that acoustic match and sequence match between the nonnative input and the native percept are optimised, simultaneously. To do so, we test a proof-of-concept computational implementation of the model as defined by \cite{wilson2013}. 
We present various methodologies for qualitatively and quantitatively evaluating the reverse inference proposal. We do this by focusing on the phenomenon of perceptual vowel epenthesis, namely the phenomenon by which listeners may hallucinate vowels when hearing nonnative speech that does not conform to the structural constraints of their native language. Of interest are both the rates of vowel epenthesis (i.e., how often do participants experience this?) and variations of epenthetic vowel quality (i.e., which vowel is hallucinated?).

Following on [CITE Peperkamp], the data arising from the computational models is compared to data from psycholinguistics experiments. In these, nonnative speech perception is evaluated using psycholinguistics paradigms which tap onto online (i.e., real-time, individual) perception of nonwords, in order to reduce the influence of confounds such as orthography and semantics.
In other words, we subject the proposed computational models to tasks analogous to those completed by human participants and analyse their behaviour both quantitatively and qualitatively. Do we find acoustics-based mechanisms to be necessary to predict perceptual vowel epenthesis? If so, do they suffice?

%Beginning from the time spent in her mother's womb, a baby is exposed to sounds and, importantly, to speech. Initially a ``universal listener'', in her early months she is able to discriminate a wide variety of sounds [CITE]. However, as she is exposed to only one or a few languages that are spoken around her, she progressively loses the ``universal listener'' title. Indeed, through this exposure, her perceptual system progressively becomes attuned to sounds and structures useful for decoding her native language(s) [CITE Werker].

%That is, at around 10 months of age, the infant's perceptual system shows signs of becoming optimised for receiving and processing native input.
%On the one hand, a native phonemic and suprasegmental inventories are established. Suprasegments are acoustic effects spanning more than one phoneme, such as stress, tone, pitch. But what are phonemes? 
%Phonemes, also refer to as segments, are generally defined as the smallest phonetic units capable of conveying a lexical distinction in a language. It is therefore essential for the infant to be able to notice when two different acoustic signals correspond to the same or different phonemes, in order to properly access lexical meaning. For this to happen, her perceptual system partitions the acoustic space into areas corresponding to the phonemes relevant for her native language [CITE]. 

%On the other hand, the infant also becomes aware of which phoneme combinations occur in the language and which ones do not [CITE Saffran, ...]. In particular, she progressively acquires the phonotactics for her native language, namely, the constraints determining what constitutes well-formed\footnote{Well-formedness can be defined within a probabilistic framework, but also with hard, binary constraints, or even a combination of the two types of processes. I do not dwell on the subtleties existing between the two in this work, however the interested reader is invited to consult the extensive review in [CITE: Lentz2011].} native sound combinations within words and syllables [CITE: Jusczyk, Pisoni?...]. This is useful, for instance, when trying to find word boundaries in fluent speech, as it is more probable for a rare phoneme combination to occur between words than within words [CITE]. 

%Through the optimisation process that results in the acquisition of the native phoneme inventory, native suprasegmentals, and native phonotactics, the infant's perceptual system becomes specialised in her native language, allowing her to better tackle problemes such as word learning.

%Years later, the infant is now an adult with an attuned perceptual system. And while this perceptual attunement allows her to communicate efficiently in her native language, issues arise when she now attempts to acquire nonnative languages.Nonnative speech is processed by her perceptual system, which has been optimised for her native language. The nonnative speech is therefore processed according to her native segmental and suprasegmental inventories, and phonotactics, which do not match those intended by the speech source. For our adult listener this will often result in nonnative speech being misperceived; phonemes and prosodic elements might be replaced (assimilation) or deleted (ellipsis). Additional phonemes might even be inserted (epenthesis).

%It seems to be generally accepted in the nonnative speech perception literature that misperceptions of nonnative speech result from minimal modifications of the original speech. However, there are multiple proposals relating to the exact nature of these modifications. For instance, are these phonetically-minimal? Phonologically-minimal? At what perceptual stage do they apply?

\section{Nonnative misperceptions}
\setlength{\epigraphwidth}{0.7\textwidth}
\epigraph{Luke: Every time you say `Luke', I think you're saying `look'. \\ Gloria: I don't hear the difference. \\ Luke: It's not that hard; one is my name. \\ Gloria: Juan is not your name!}{\textsc{Modern Family}}

%{\color{red}[Sharon]: 
%  You should talk about the difficulties adults have perceiving (and if you wish producing) foreign sounds and sound structures! An anecdote would be ok (but not necessary), and then explain that our perception is optimized for native language processing and - if you wish! - that this is put in place during the first year of life.}

Let us paraphrase a question asked in the previous section, which is central to the field of nonnative speech perception: Why do individuals with different language experiences interpret the same acoustic signal differently? To understand how this mismatch occurs, it is necessary to keep in mind what the perceptual system is tasked to do. That is, during speech perception the perceptual system is tasked to map a continuous acoustic signal to discreet phonemic categories defined by the language's phonemic inventory, and being aware of language suprasegmentals (e.g., pitch, tones, lexical stress) and phonotactics (i.e., legal sound structures in a language). It so happens that the inventory of phonemes, suprasegments, and phonotactics, are progressively set in stone following exposure to the native language(s) during the first year of life [CITE].

Experimental investigations in the area of nonnative misperceptions set sail with early investigations by \cite{polivanov1939, trubetzkoy1939}, where the authors posited that listeners incorrectly parsed the nonnative input with an incompatible set of native phonological contrasts. Several years afterwards, nonnative misperceptions have been documented in multiple native-nonnative language combinations (e.g., [CITE]). Models of nonnative speech (mis)perception explain the phenomenon as resulting from different weighting of acoustic dimensions or cues [CITE], from attempted parsing of a nonnative input with inadequate native contrastive phonetic [CITE] or phonological features [CITE], or from mapping of the input to native categories that do not share the same contrasts and/or boundaries as in the intended nonnative categories \cite{best1994, kuhl1995}. 

These models were developed mostly with regards to phonemes. However, misperceptions can be classified in three main categories:

\begin{itemize}
\item Segmental misperceptions: Difficulties perceiving the difference between two nonnative contrasting phonemes. E.g., Japanese listeners' difficulty perceiving the difference between American English \textipa{/r/} and \textipa{/l/} \cite{goto1991, miyawaki1981}.
\item Suprasegmental misperceptions: Difficulties perceiving the difference between two nonnative contrasting suprasegments such as lexical stress, lexical tone, pitch accent. E.g., French listeners' difficulty perceiving the difference between Spanish words such as ``líquido'' \textipa{/'likido/} (\textit{liquid}), ``liquido'' \textipa{/li'kido/} (\textit{I liquidate}), ``liquidó'' \textipa{/liki'do/} (\textit{s/he liquidated}) \cite{dupoux1997, dupoux2008}.
\item Phonotactic repairs: Addition (i.e., epenthesis), modification (i.e., adaptation), or deletion (i.e., elipsis) of segments as a strategy to ``fix'' nonnative input which does not conform to native phonotactics. E.g., Word-initial \textipa{/e/}-epenthesis by Spanish speakers in words beginning with an \textipa{/s/} as part of a complex cluster as in ``special'' \cite{halle2014}. Also: Perception of illegal \textipa{/tl/} clusters as \textipa{/kl/} by French listeners \cite{halle2007}.  
\end{itemize}

Interestingly, while they can be minimised with training [CITE lively93 etc], the effects of native phonology on nonnative speech perception are long-lasting [CITE takagi95] and may even be apparent in highly proficient bilinguals [CITE FrEsp].   

\section{Perceptual vowel epenthesis}

%In the late nineties various theories of nonnative speech perception [CITE Best1995, Kuhl1995] and second language phoneme acquisition [Flege1995] surfaced in the psycholinguistics literature. There seemed to be an overlap between the observed nonnative speech misperceptions and patterns observed in loanwords; may perception directly account for loanword adaptation?   
%It is in this context that appeared proposals such as that of [CITE PeperkampDupoux2003], which put greater emphasis on the role of misperception on loanword adaptation. The proposal being that, when perceiving nonnative input, a phonetic decoding module takes into account segmental, suprasegmental, and syllabic inventories of the native language in order to derive a phonetically-minimally modified representation\footnote{Whether this representation is acoustic, articulatory [CITE cf Berent2015], and/or gestural [CITE: BrowmanGoldstein, Best1995, BestTyler2007] in nature is not discussed here.} of the acoustic input. It is this misperceived version that is then converted into an underlying representation by a phonological decoding module; this cemented underlying representation is unfaithful with respect to the original underlying representation in the nonnative language.  
%In this context,
In this thesis we will focus on a subset of misperceptions resulting from phonotactic repair: perceptual vowel epenthesis. As previously mentioned, we say that listeners experience this phenomenon when they hallucinate vowels not initially present in the nonnative speech. In cases that will be studied in this work, this seemingly happens as a way to break phonotactically illegal clusters. For instance, in Japanese most consonant clusters%\footnote{Consonant clusters can only be geminates or a nasal followed by another consonant.}
, such as \textipa{/bz/}, are phonotactically illegal. When hearing nonwords containing these clusters, such as \textipa{/ebzo/}, Japanese listeners may hallucinate an epenthetic \textipa{/u/}\footnote{A more accurate phonetic transcription of the unrounded high back vowel used in Japanese is \textipa{[W]} but, following previous work, the phonological notation \textipa{/u/} will be used in the remainder of the thesis.} within the cluster, yielding \textipa{/ebuzo/} as the percept \cite{dupoux1999}. Epenthesis of \textipa{/u/} by Japanese listeners is not only evident in their behaviour but also in their brain responses; they have difficulties differentiating the clusters produced by a French speaker from their epenthesized counterparts (e.g., \textipa{/ebzo/} \textit{vs.} \textipa{/ebuzo/}) while also showing different event-related potentials compared to native French speakers. The fact that Japanese listeners fail to show sign of MMN (mismatch negativity) in the EEG signal attests that the process of epenthesis occurs early in the process of perception \cite{dehaene2000}. Importantly, experimental data also suggests that epenthesis is a pre-lexical process happening early in speech perception \cite{dupoux2001}.

Perceptual vowel epenthesis has also been attested in languages other than Japanese \cite{dupoux1999, dehaene2000, dupoux2001, monahan2009, dupoux2011, mattingley2015}; indeed, it has also been studied in Korean \cite{kabak2007, shin2011, dejong2012, durvasula2015, durvasul2016}, Brazilian Portuguese \cite{dupoux2011}, Spanish \cite{halle2014}, English [CITE: Berent (multi), Davidson], and Mandarin Chinese \cite{durvasula2018}. The quality of the epenthetic vowel depends not only in the language (e.g., \textipa{[W]} in Japanese, \textipa{[i]} in Brazilian Portuguese \cite{dupoux2011}), but also in the phonemic environment (e.g., \textipa{/i/} may be more readily epenthesized in clusters with palatalised consonants). 

%While research on perceptual vowel epenthesis uses the literature on loanword adaptation as a a source of inspiration and a source of informed predictions, it is important to note that the phenomenon of vowel epenthesis is not defined equally in both fields. Remember that loanword adaptation is a diachronic process, involving complex interactions between several groups of individuals, with varying degrees of source language fluency. 
The psycholinguistic approach to vowel epenthesis focuses on online perception and, while data from multiple participants is collected, the modifications observed on the output of perception are assumed to occur at the level of the individual, as there is no interaction between participants. Participants are not expect to be proficient in the nonnative language, which minimises the influence of linguistic knowledge on misperceptions (e.g., orthography). 

%  - Shin \& Iverson (2011): correlation between \%epenth and vowel discrimination.   \\
% - Kabak \& Idsardi (2007): a phonological influence of L1 phonotactic knowledge, rather than an effect of frequency, plays a primary role in explaining KR groups performance."
%- Browman \& Goldstein (1980s): Articulatory Phonology; dynamic gestures as units, developing in space at time (as opposed to the static segment). Sounds correspond to constellation of gestures. Allows to explain gradient effects.  
%- Zhao \& Berent (2018): Epenthesis from read stimuli (i.e., no acoustic input) \\
%- Best \& Tyler (2007): "PAM posits that perceivers extract invariants about *articulatory gestures* from the speech signal, rather than forming categories from acoustic-phonetic cues"

\section{Processing steps in perceptual vowel epenthesis}

% From now on we will focus on perceptual vowel epenthesis, referring to loanwords as a source of inspiration for experimental setups and as a source of informed predictions.
Concerning the process of vowel epenthesis, we can identify two types of proposed pipelines that differ in the amount of processing steps that the nonnative input is subjected to during perception: these are two-step and one-step theories of nonnative speech perception, illustrated in Figure \label{ref:intro_12step}. While their names are somewhat transparent, we will now explain in more detail the differences between the two types of proposals.   

% Reminiscent of Silverman's multiple scansion theory for loanword adaptations \cite{silverman1992},
Two-step theories of nonnative speech perception divide the perception process in two stages. According to these proposals, the quality of the epenthetic vowel is determined by a language-specific grammar after an initial parsing of the nonnative input.
For \cite{berent2007}, the identity of the segments present in the nonnative input is retrieved in an initial step, yielding a \textit{phonetic form}. The native grammar then assesses the phonotactic legality of this phonetic form in a second step. If a phonotactic violation is found, the grammar, which combines both language-specific and universal components, repairs the phonetic form by inserting a vowel. The output of this final step is the \textit{phonological representation}.
Another proposal, that of \cite{monahan2009}, also consists in two steps, but with some differences. During the first step the identity of the segments in the input is retrieved and segments are grouped into syllables, following native phonotactics. Some syllables will contain indeterminate segments (e.g., \textipa{/ebzo/} will have been parsed as \textipa{/e.b}V\textipa{.zo/}). In a second step, the quality of the indeterminate segments, in this case the epenthetic vowel, is chosen amongst vowels that are of low sonority\footnote{Phonemes that are lower in the sonority scale are less audible than higher ranked phonemes. Due to how the tongue is positioned close to the mouth roof during their articulation, high vowels (and glides) are the least sonorous vowels in a vowel inventory.} and can undergo devoicing\footnote{In Japanese, high vowels \textipa{/i/} and \textipa{/u/} can be devoiced in certain contexts, such as between two voiceless segments \cite{han1962,vance1987, tsuchida2001}.}. 
The quality of the vowel might not be determined if an optimal match is not found.
The two proposals that we have summarised share the fact that the categorisation of the segments that are not the epenthetic vowel occurs in a first step and it is not modified during the second step, where the identity of the epenthetic vowel is determined.

\begin{figure}[htb!]
  \centering
  \begin{overpic}[page=1, width=0.9\linewidth]{chapter01-intro/12step}\end{overpic}
  \caption{\textit{Processing of the nonnative stimulus \textipa{/ebzo/} by Japanese listeners, according to two-step and one-step proposals for perceptual vowel epenthesis. From left to right: two-step proposal from \cite{berent2007}, two-step proposal from \cite{monahan2009}, one-step proposal from \cite{dupoux2011,dejong2012,wilson2013}}}
  \label{fig:intro_12step}
\end{figure}

In contrast, by advancing one-step proposals, authors such as \cite{dupoux2011, dejong2012} and \cite{wilson2013} argue that the identity of the epenthetic vowel is determined in the process of parsing the input, simultaneously to the categorisation of all other segments. The phonotactic legality of the input is therefore assessed at the same time as the categorisation happens. Notably, the input is not processed as a linear sequence of sounds; syllabic structure is taken into account during the parsing process \cite{kabak2007}.

\cite{wilson2013} qualify the process as a process of reverse inference within a Bayesian framework, where the perceptual system computes $P(w | X)$ the posterior probability of candidate percepts $w$ given the auditory input $X$. These are estimated, for each candidate percept, from the product of $P(X|w)$ the likelihood of the acoustics given the percept and $P(w)$ the prior probability of the percept, defined as its phonotactic acceptability. Mathematically, this can be formulated as in equation \ref{eq_onestep1}. Then, in a maximum \textit{a posteriori} (MAP) estimation scenario, the final percept $\widehat{w}$ corresponds to the percept with the highest posterior probability, as shown in equation \ref{eq_onestep2}. Alternatively, the final percept may be estimated by weighted sampling, where weights are defined by the posterior probabilities.     

\begin{equation}
  P(w | X) \propto P(X | w) \cdot P(w)
  \label{eq_onestep1}
\end{equation}

\begin{equation}
  \widehat{w} = \underset{w}{arg\,max} \left \{ P(X|w) \cdot P(w) \right \}
  \label{eq_onestep2}
\end{equation}

In other words, for one-step models, parsing becomes an optimisation problem where the optimal output is the one maximising the acoustic match to the input and the likelihood of the phonemic sequence in the native language. \cite{durvasula2015} add to the aforementioned proposals by suggesting that listeners are decoding nonnative speech through a process of reverse inference that not only optimises the output according to phonetic representations and surface phonotactics, but also according to native phonological alternations (i.e., mappings between underlying and surface representations). What this means is that listeners will also try to infer an underlying phoneme based on the possible surface realisations attached to this phoneme in their native language. For instance, in Korean, \textipa{/s/} surfaces as \textipa{[S]} when in front of the vowel \textipa{/i/}. When hearing a cluster such as \textipa{[Sm]}, listeners may epenthesize \textipa{[i]} after interpreting \textipa{[S]} as the allophone of \textipa{/s/} in front of that vowel. Thus, the extended proposal by \cite{durvasula2015} integrates the involvement of deeper phonological rules/constraints during the perception process.

\section{Vowel epenthesis in loanword adaptations}
%{\color{red}[Sharon]: I'd change the order: psycholing. first, and then - because of lack of data - loanwords (which otherwise come out of the blue) \\
%Alternatively, if you want to start with loanwords, you should introduce them in the very first paragraph of the intro, which should then go something like this: \\
%- we have an accent in L2 \\ 
%- we adapt loanwords \\
%- both are production phenomena that are at least in part due to speech perception}
The phenomenon of vowel epenthesis has also been studied in the context of loanword adaptations, namely the modification of words from a source language when they are introduced to a borrowing language. Consider the bold vowels in the following examples:
\begin{itemize}
  \item English ``strike'' \textipa{/st\*raIk/} $\rightarrow$ Japanese \textipa{/s\textbf{u}t\textbf{o}raik\textbf{u}/}
  \item French ``baguette'' \textipa{/bagEt/} $\rightarrow$ Japanese \textipa{/baget:\textbf{o}/} %\begin{CJK}{UTF8}{goth}バゲット\end{CJK} \textipa{//} 
  \item English ``snob'' \textipa{/sn6b/} $\rightarrow$ Spanish \textipa{/\textbf{e}snob/}
  \end{itemize}
  
In this context, vowel epenthesis consists in the insertion of a vowel in the (borrowing) surface form of a word that did not contain said vowel in the underlying (or surface) representation in the source language. Epenthesis often occurs when the introduced word does not respect the phonotactics of the borrowing language (e.g., illegal consonant clusters, illegal syllabification).
The foreign word is imported to the borrowing language in a diachronic process involving multiple individuals and possibly various methods of word transmission (e.g., through written materials, orally, etc). Additionally, adaptations can be influenced by orthography, when available [CITE Daland 2015; Vendelin\&Ppkmp, Ito?]. In this framework the assumption often is that words are introduced to the borrowing language by highly proficient speakers of the source language that are, therefore, able to access a faithful underlying representation of the source word. 

Concerning the question of how the nonnative source word is transformed into the adapted loanword, authors from the loanword literature advance the hypothesis that the underlying mechanisms are phonological and abstract in nature.

For instance, in a rule-based view [CITE SPE], authors hypothesized the existence of phonological rules applied to nonnative phonemic structures in order to obtain an output (i.e., the resulting loanword) that conforms to native phonotactics \cite{hyman1970, lovins1975}.
Similarly, in a grammar-based view [CITE OT Prince\&Smolensky], the adapted loanword corresponds to the best match to the source word after candidate adaptations are passed through a grammatical filter, in which ranked constraints select an optimal output [CITE: Yip (1993), Jacobs \& Gussenhoven (2000), Shinohara (2004)]. %In this case, {\color{red}[TODO; unclear] the grammar comprises faithfulness and markedness constraints arranged in a certain order, with some authors suggesting that some modifications might be more degrading that others \cite{steriade2001}. How constraints are arranged may or may not be compatible with how native sets of constraints are ordered.}
It is assumed that the underlying representation of the source word is accessible and is used in the derivation of the adapted loanword. As such, the faithful underlying representation only becomes adapted when the grammar computes a surface representation from said underlying form (e.g., during production).   \\

Another similar mechanism was proposed by [CITE La Charité \& Paradis (1997, 2005)], where adaptations are based on minimal featural changes. The source word is adapted by highly proficient bilinguals, who can manipulate and compare the phonological systems of both the source and the borrowing languages. In this case, adaptation consists in these highly proficient bilinguals choosing the modifications of the surface form of the source word that result in the least featural changes between the source adaptation and the resulting loanword.  

In many proposals, the role of perception has been assumed to be minimal, or at least secondary, in loanword adaptation [CITE e.g., LaChParadis1997]. However, this view is far from being a consensus. For instance, \cite{steriade2001} proposed that some grammar-based modifications resulted in more perceptually-salient modifications of the source word, resulting in a greater amounts of degradation of the output with respects to the input.

Perception is also an integral part of the multiple scansion theory of loanword adaptation proposed by [CITE silverman1992]. In this theory, the underlying representation in the borrowing language is retrieved from a phonetic, possibly acoustic, form of the source word. In a first step, a perceptual-level representation is established, where the underlying representation is built with preliminary segmental and prosodic structure. It is only at a second step, at the operational level, that the phonology of the borrowing language is applied to the word (e.g., through a grammatical filter), resulting in modifications such as vowel epenthesis, when necessary. For similar proposals, see [CITE Kenstowicz 2001/2003, KenstowiczSuchato, Yip1993/2006].
Going even further, [CITE Peperkamp05 \cite{peperkamp2005}] proposed that all loanword adaptations result from phonetically minimal transformations of the nonnative input, taking place during perception.

If patterns of epenthesis observed in loanword adaptations are, at least in part due to how the native perceptual system processes the nonnative source word, loanwords can be thought of as fossils from which we can extract hypotheses about online misperceptions. Of course, as we mentioned above loanwords are processed through more than one individual perceptual filter, and can be influenced by orthography, for instance. So, continuing with the fossil metaphore, the online percept is akin to the mosquito trapped in amber after accidentally landing on tree sap; preserved in time but possibly not in mint condition. Large corpora of loanwords exist, which allows us to examine epenthetic patterns: Which phonemic combination trigger vowel epenthesis? Is there a vowel that is predominantly inserted? If so, are other vowels epenthesized in other phonemic environments? Similar to how the paleontologist posits theories about the fauna in ancient times by analysing fossils, the psycholinguist is able to posit theories about perceptual vowel epenthesis from epenthesis in loanwords, and test them empirically.      

\section{Justifying the modelling approach}
\setlength{\epigraphwidth}{0.4\textwidth}
\epigraph{... But why make models?}{\textit{Derek Zoolander, probably.}}

%{\color{red}[Sharon]: ``cette section est très courte. Ne peux-tu pas donner une synthèse historique sur l'utilisation de la modélisation en perception de la parole (ou, su tu veux, perception en général, psychophysique, qu'en sais-je...)''}

In this thesis we will evaluate computational implementations of one-step theories of nonnative speech perception. Notably, we will investigate models from the field of automatic speech recognition (ASR) which are a direct implementation of the Bayesian model shown in equation \ref{eq_onestep1}\footnote{The equivalent of a weighted sampling procedure was preferred over MAP estimation for percept selection, since participant responses in previous experimental work on epenthesis tended to show variation and were not deterministic.}. 
While only the one-step family of theories will be evaluated in this work, we encourage further research to be done with similar methodologies in order to investigate all of the various co-existing proposals.

Indeed, using computational models in order to investigate competing theories is beneficial in several ways. Firstly, the need to translate the theories into model implementations forces model ideators to provide a mathematically and/or algorithmically well-defined model. This is in contrast with more vague and ambiguous verbally defined theories that leave more space to reader interpretations. Having more rigorous model definitions also allows us to better understand competing theories (what is the exact nature of the input? which grammar constraints are applied and how? ...), meaning that it is easier to compare proposals and see where they differ significantly or not. 

Secondly, obtaining a computational implementation of a theory means that it is then possible to derive predictions from the models in question. It is then possible to qualitatively and quantitatively examine these predictions, and compare them to what is observed in behavioural data.

For the skeptical reader, possibly left frowning after reading the above statements, we will briefly develop an example of how modelling can allow us to test theories in ways that may be unfeasible otherwise. The specific example, the details of which can be found in Appendix \ref{A-appendix}, is from the literature of developmental psycholinguistics. In a series of studies, we examined the hypothesis that the acoustic characteristics of infant-directed speech (IDS) enhanced language learning in infants, compared to adult-directed speech (ADS). Indeed, IDS presents very salient prosodic, lexical, syntactic, and temporal properties (see [CITE Soderstrom2007, Golinkoff2015] for a review).

The hypothesis (which we refer to as the Hyper Learnability Hypothesis; HLH) was advanced by \cite{kuhl1997}. The authors in this study analysed the acoustics of the vowels located at the extremities of the vowel triangle (i.e., \textipa{/i, a, u/}), produced by speakers of American English, Russian, and Swedish. Analyses showed an increase of the vowel triangle area (in formant space) for IDS compared to ADS. The authors interpreted the increased separation of IDS vowels as an enhancement of phonemic contrasts, which might help infants identify and acquire phonemic categories more easily. The expansion of the vowel triangle in IDS was also attested in other studies \cite{andruski1999, bernstein1984, burnham2002, cristia2014, liu2003, mcmurray2013, uther2007}. However, increased separation was not systematic across the vowel inventory \cite{cristia2014} and, importantly, IDS presented increased within-category variability \cite{mcmurray2013, cristia2014, kirchoff2005}. With increased vowel separation and increased within-category variability being opposite effects, we wondered whether the discriminability of IDS phonemes was higher than for ADS vowels or not.

Using a computational model of the ABX discrimination task\footnote{In this task, the discriminability of two categories \textsc{A} and \textsc{B} is assessed by setting triplets of tokens $a$, $b$, and $x$. The first two belong to categories \textsc{A}, \textsc{B}, respectively; the third token belongs to one of the two categories. The algorithm computes the acoustic distance between $a$ and $x$, and $b$ and $x$, and classifies $x$ to the category of the closest token. The more discriminable the two categories in question, the higher the classification accuracy of the algorithm.}, and either \cite{martin2015} assessed the discriminability of Japanese phonemes in a large corpus of Japanese IDS and ADS. Against expectations, phonemes in IDS were on average less discriminable that in ADS. In the Appendix \ref{A-appendix}, we investigated whether the acoustic and phonological advantage of IDS may surface at level of words. However, we found that words in IDS were also less discriminable than in ADS on average. 

In this set of studies, the modelling approach allowed us to quantitatively and qualitatively test the HLH, in a large scale (several millions of ABX experimental trials simulated), with systematic comparisons of all phonemes/words in the language, without assuming a specific learning algorithm, and using a richer representation of the acoustics based on a model of how speech is processed by the auditory system (as opposed to formant values in other studies). Importantly, modelling allowed us to evaluate the interaction between effects advancing opposing hypotheses, and showed us the resulting predictions in a computationally understandable format. A study of this magnitude would have not been possible with traditional experimental techniques, which makes modelling a welcomed addition to the evidence gathered for and against the HLH.  


\section{Outline}
This dissertation is divided in two main sections. First, in Chapter 2, we use an identification paradigm to investigate the influence of acoustic details on modulations of epenthetic vowel quality. We discuss the implications of our results in the context of the opposition between the two-step and one-step theories of nonnative speech perception. We find that acoustic details modulate epenthetic vowel quality, results that are in agreement with one-step theories. Building on these results, we present a basic model of speech perception exclusively reliant on acoustic matching betwen minimal pairs of nonnative and native speech exemplars. Namely, we build non-parametric exemplar-based models of perception. Relative to human results, we find the models to be able to reproduce some qualitative effects linked to the role of coarticulation on epenthetic vowel quality; however, the models are limited by their inability to output responses other than those derived from their specific inventory of exemplars.

In Chapter 3 we turn to a parametric implementation of a one-step proposal, using tools from the field of automatic speech recognition (ASR). We present an HMM-GMM speech recognizer composed of independent acoustic and phonotactic models. These can be tweaked as necessary to test hypotheses about the underlying mechanisms of nonnative speech perception. We propose a novel methodology to test ASR systems which use language models represented by Weighted Finite State Transducers (W-FST) in identification tasks analogous to those used to test human participants. Using this method, we test the predictive power of the acoustic model on patterns of vowel epenthesis. We find that the acoustic model alone better predicts human results than when accompanied by language models, at least when the latter are \textit{n}-gram based phonotactic models. We further test whether some effects traditionally attributed to phonology may actually be predicted from acoustics alone. Following promising but not perfect results, we propose future research paths for enhancing the methodology and to further investigate the mechanisms underlying nonnative speech perception.  